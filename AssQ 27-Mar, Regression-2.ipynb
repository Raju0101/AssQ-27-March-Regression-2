{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414f20b0-5a79-4808-8af9-fcc768bd5ff5",
   "metadata": {},
   "source": [
    "# AssQ 27-March, Regression-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668d777-ad1f-4d0f-b2f7-26ee7e3e4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it  represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc755c2-3d48-48bd-9c46-a7832158576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that\n",
    "determines the proportion of variance in the dependent variable that can be explained by the independent variable. \n",
    "In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "\n",
    "An R-squared value will always range between 0 and 1. What is this? A value of 1 indicates that the explanatory \n",
    "variables can perfectly explain the variance in the response variable and a value of 0 indicates that the \n",
    "explanatory variables have no ability to explain the variance in the response variable.\n",
    "\n",
    "R-squared measures the strength of the relationship between your linear model \n",
    "and the dependent variables on a 0 - 100% scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda80852-40b8-4bdb-aef5-cd8ea35c591f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e09740-123a-457f-aef7-c8ba0c52eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378322d-3d1f-47f1-a73e-cf0b9c81791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
    "The adjusted R-squared increases when the new term improves the model more than would be expected by chance.\n",
    "It decreases when a predictor improves the model by less than expected.\n",
    "\n",
    "The difference between R squared and adjusted R squared value is that R squared value assumes that all the \n",
    "independent variables considered affect the result of the model, whereas the adjusted R squared value considers\n",
    "only those independent variables which actually have an effect on the performance of the model.\n",
    "\n",
    "The adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant\n",
    "in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a8fa3-38e4-4f21-b021-2cba42962a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09a81e-0dfe-40f7-9c4d-047c1725c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ed81c-3cbb-401b-949a-d93deb7f3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-Squared Examples. When you are analyzing a situation in which there is a guarantee of \n",
    "little to no bias, using R-squared to calculate the relationship between variables is perfectly useful.\n",
    "\n",
    "Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model.\n",
    "This would allow us to compare models with differing numbers of independent variables.\n",
    "\n",
    "the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\n",
    "\n",
    " R-squared increases every time you add an independent variable to the model. The R-squared never decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730cdea-caf2-4f63-82b5-70857a2063f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e4900-ee0a-42e7-a17a-6b88b52e694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics \n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdd1e5-8042-4095-84a4-416b1b210312",
   "metadata": {},
   "outputs": [],
   "source": [
    "Usually the metrics used are the Mean Average Error (MAE), the Mean Squared Error (MSE)\n",
    "or the Root Mean Squared Error (RMSE). In short, MAE evaluates the absolute distance of\n",
    "the observations (the entries of the dataset) to the predictions on a regression,\n",
    "taking the average over all observations.\n",
    "\n",
    "The Root Mean Squared Error (RMSE) is one of the two main performance indicators for a regression model.\n",
    "It measures the average difference between values predicted by a model and the actual values.\n",
    "It provides an estimation of how well the model is able to predict the target value (accuracy).\n",
    "\n",
    "Mean Absolute Error (MAE)-\n",
    "It is calculated by taking the absolute difference between the predicted values and the actual values\n",
    "and averaging it across the dataset. Mathematically speaking, it is the arithmetic average of absolute errors.\n",
    "\n",
    "The RMSE result will always be larger or equal to the MAE. If all of the errors have the same magnitude,\n",
    "then RMSE=MAE. [RMSE] ≤ [MAE * sqrt(n)], where n is the number of test samples. The difference between \n",
    "RMSE and MAE is greatest when all of the prediction error comes from a single test sample.\n",
    "\n",
    "The root mean squared error (RMSE) would simply be the square root of the MSE: RMSE = √MSE.\n",
    "\n",
    "Root Mean Squared Error (RMSE)and Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. \n",
    "These metrics tell us how accurate our predictions are and, what is the amount\n",
    "of deviation from the actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fc00c-f1f8-400f-a2de-afbfa75c2b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5045d-f30f-455a-a90e-1ebfcf829d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in \n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea3eda-a788-4366-8a57-fdcd5040e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, \n",
    "since the MSE puts larger weight on theses errors due to the squaring part of the function.\n",
    "RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, \n",
    "for example, if being off by 10 is more than twice as bad as being off by 5. But if being off\n",
    "by 10 is just twice as bad as being off by 5, then MAE is more appropriate.\n",
    "\n",
    "Disadvantage: If our model makes a single very bad prediction, the squaring part\n",
    "of the function magnifies the error.\n",
    "One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed \n",
    "for it to function properly. RMSE increases with an increase in the size of the test sample.\n",
    "This is an issue when we calculate the  results on different test samples.\n",
    "\n",
    "Disadvantages of MAE: \n",
    "i) Beacuse of its non-differentiable nature of graphs,\n",
    "will need various optimizers like gradient descent to make them differentiable.\n",
    "ii) Bigger error terms are failed to be punished. Mean of the squared distances\n",
    "between actual and predicted values.\n",
    "\n",
    "Although it is straightforward to interpret, MAE has a few disadvantages. For example,\n",
    "it doesn't tell you whether your model tends to over-estimate or under-estimate since any direction\n",
    "information is destroyed by taking the absolute value.\n",
    "Also, the metric can be insensitive to large outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2b06b-c160-43b4-90c5-7a54098c5716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8fdb1-ea7f-4459-beb8-134c3d8dd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is \n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efdf0c-c6ad-455d-a87d-f22e84e4c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. \n",
    "This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\n",
    "The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n",
    "Lasso regression is also referred to as L1 Regularization.\n",
    "\n",
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing\n",
    "a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "Lasso method is usually used in machine learning for the selection of the subset of variables.\n",
    "It provides greater prediction accuracy as compared to other regression models. \n",
    "Lasso Regularization helps to increase model interpretation. \n",
    "The less important features of a dataset are penalized by the lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10016b-01a3-4f8a-b382-b53d2415b8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523bcbd-88f2-4bea-a01e-58ca60783e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an \n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c09f11-4618-4e93-b8db-28a37ff40b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "An example is noise. Regularization is the answer to overfitting. \n",
    "It is a technique that improves model accuracy as well as prevents the loss of important data due to underfitting.\n",
    "When a model fails to grasp an underlying data trend,\n",
    "it is considered to be underfitting.\n",
    "\n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain, \n",
    "regularizes, or shrinks the coefficient estimates towards zero. In other words,\n",
    "this technique discourages learning a more complex or flexible model,  avoiding the risk of Overfitting.\n",
    "\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to \n",
    "minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, \n",
    "we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d282f9-7229-4842-8aa9-3081801a18c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181c7b4-4c94-46cf-8977-60b01aa135f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best \n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37671c90-0a24-4eb2-9b65-ac9e171ce08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Main limitation of Linear Regression is the assumption of linearity between the dependent variable \n",
    "and the independent variables. In the real world, the data is rarely linearly separable.\n",
    "It assumes that there is a straight-line relationship between the dependent \n",
    "and independent variables which is incorrect many times.\n",
    "\n",
    "Regularization leads to dimensionality reduction, which means the machine learning model is built using\n",
    "a lower dimensional dataset. This generally leads to a high bias errror. \n",
    "If regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used.\n",
    "\n",
    "It involves very lengthy and complicated procedure of calculations and analysis. \n",
    "It cannot be used in case of qualitative phenomenon viz. honesty, crime etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908e4d1-bf05-4b60-b31f-1684a12f94ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8c752-81c0-45a4-beae-c8452a95bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. \n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better \n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e4326-7d99-447f-b6b9-a7839f6b5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "we will choose MAE of 8  as the better performer.\n",
    "\n",
    "The RMSE result will always be larger or equal to the MAE. If all of the errors have the same magnitude, \n",
    "then RMSE=MAE. [RMSE] ≤ [MAE * sqrt(n)], where n is the number of test samples. \n",
    "The difference between RMSE and MAE is greatest when all of the prediction error comes from a single test sample.\n",
    "\n",
    "Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. \n",
    "This means the RMSE is most useful when large errors are particularly undesirable.\n",
    "Both the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f17bf-482c-4245-b08a-837a7fe12722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9095397-e11d-431d-bc68-466d4a6a3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of \n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B \n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the \n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization \n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ab84a-5040-42d0-afaa-f63536da9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "A regression model that uses L1 regularization technique is called Lasso Regression and\n",
    "model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term.\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
    "\n",
    "L1 regularization is more robust than L2 regularization for a fairly obvious reason.\n",
    "L2 regularization takes the square of the weights, so the cost of outliers present in the data increases exponentially.\n",
    "L1 regularization takes the absolute values of the weights, so the cost only increases linearly.\n",
    "\n",
    "From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly.\n",
    "L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. \n",
    "L2, on the other hand, is useful when you have collinear/codependent features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
